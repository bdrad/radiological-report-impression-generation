{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import regex as re\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from datasets import Dataset\n",
    "import random\n",
    "from datasets import load_metric\n",
    "from string import punctuation\n",
    "import nltk.data\n",
    "from tokenizers import AddedToken\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def print_title(title):\n",
    "    print('=' * 30)\n",
    "    print(title)\n",
    "    print('=' * 30)\n",
    "\n",
    "class AIGDataset(Dataset):\n",
    "    def __init__(self,dataset,tokenizer,source_len,summ_len):\n",
    "        self.dataset = dataset \n",
    "        self.tokenizer = tokenizer\n",
    "        self.text_len = source_len\n",
    "        self.summ_len = summ_len\n",
    "        self.text = self.dataset['Source']\n",
    "        self.summary = self.dataset['Impression']\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.text)\n",
    "\n",
    "    def __getitem__(self,i):\n",
    "        summary = '<pad> ' + str(self.summary[i])\n",
    "        text = '<pad> ' + str(self.text[i])\n",
    "        source = self.tokenizer.batch_encode_plus([text],max_length=self.text_len,return_tensors='pt',pad_to_max_length=True, truncation=True) # Each source sequence is encoded and padded to max length in batches\n",
    "        target = self.tokenizer.batch_encode_plus([summary],max_length=self.summ_len,return_tensors='pt',pad_to_max_length=True, truncation=True) # Each target sequence is encoded and padded to max lenght in batches\n",
    "\n",
    "\n",
    "        source_ids = source['input_ids'].squeeze()\n",
    "        source_masks = source['attention_mask'].squeeze()\n",
    "        target_ids = target['input_ids'].squeeze()\n",
    "        target_masks = target['attention_mask'].squeeze()\n",
    "\n",
    "\n",
    "        return {\n",
    "            'source_ids':source_ids.to(torch.long),\n",
    "            'source_masks':source_masks.to(torch.long),\n",
    "            'target_ids':target_ids.to(torch.long),\n",
    "            'target_masks':target_masks.to(torch.long)\n",
    "        }\n",
    "    \n",
    "def test(tokenizer,model,device,loader):\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    with torch.no_grad():\n",
    "        progress_bar = tqdm.tqdm(loader)\n",
    "        for data in progress_bar:\n",
    "            ids = data['source_ids'].to(device)\n",
    "            mask = data['source_masks'].to(device)\n",
    "            y_id = data['target_ids'].to(device)\n",
    "            prediction = model.generate(input_ids=ids,attention_mask = mask,num_beams=2,max_length=200,repetition_penalty=1.5,early_stopping=False,length_penalty=1.0)\n",
    "\n",
    "            # Decode y_id and prediction #\n",
    "            source = [tokenizer.decode(s,skip_special_tokens=True,clean_up_tokenization_spaces=False) for s in ids]\n",
    "            preds = [tokenizer.decode(p,skip_special_tokens=False,clean_up_tokenization_spaces=False) for p in prediction]\n",
    "            target = [tokenizer.decode(t,skip_special_tokens=False,clean_up_tokenization_spaces=False) for t in y_id]\n",
    "\n",
    "            predictions.extend(preds)\n",
    "    return predictions\n",
    "\n",
    "def make_demo(mode, reader_performance, department=None, zsfg=False):        \n",
    "\n",
    "    processed_data = pd.read_csv(f'data/processed/{mode}_test_dataset.csv').sample(frac=1)\n",
    "    if zsfg:\n",
    "        processed_data = pd.read_csv(f'data/processed/zsfg_{mode}_test_dataset.csv').sample(frac=1)\n",
    "    processed_data = processed_data.drop_duplicates(subset=['Impression'], ignore_index=True).dropna()\n",
    "    if reader_performance:\n",
    "        processed_data = processed_data[~processed_data['Impression'].str.contains('biopsy')]\n",
    "    if department:\n",
    "        processed_data = processed_data[processed_data['Exam'].str.startswith(department)]\n",
    "\n",
    "    tokenizer = T5Tokenizer.from_pretrained('t5-base')\n",
    "    tokenizer.add_special_tokens({\"additional_special_tokens\": [AddedToken(\"\\n\")]})\n",
    "    dataset = Dataset.from_pandas(processed_data.reset_index(drop=True))\n",
    "    \n",
    "    test_dataset = AIGDataset(dataset,tokenizer,400,200)\n",
    "    test_loader = DataLoader(dataset = test_dataset,batch_size=1,num_workers=0, shuffle=False)\n",
    "\n",
    "    model = T5ForConditionalGeneration.from_pretrained('t5-base')\n",
    "    model.load_state_dict(torch.load(f'models/aig_t5_weights_{mode}'))\n",
    "    model = model.to(device)\n",
    "\n",
    "    predictions = test(tokenizer,model,device,test_loader)\n",
    "    predictions = [postprocess(prediction) for prediction in predictions]\n",
    "    processed_data['Predicted Impression'] = predictions\n",
    "                  \n",
    "    if reader_performance:\n",
    "        processed_data = processed_data.drop_duplicates(subset=['Predicted Impression'], ignore_index=True).dropna()[:50]\n",
    "    return dataset['Findings'], dataset['Impression'], predictions, processed_data\n",
    "\n",
    "def postprocess(prediction):\n",
    "    prediction = prediction.replace('<pad>', '')\n",
    "    prediction = prediction.replace('</s>', '')\n",
    "    prediction = prediction.replace(' \\n ', '\\n')\n",
    "    prediction = prediction.strip()\n",
    "    return prediction\n",
    "\n",
    "def demo(source, actual, predictions):\n",
    "    i = random.randint(0, len(source))\n",
    "    print('SOURCE:')\n",
    "    print(source[i])\n",
    "    print('')\n",
    "    print('ORIGINAL IMPRESSION:')\n",
    "    print(actual[i])\n",
    "    print('')\n",
    "    print('PREDICTED IMPRESSION:')\n",
    "    print(predictions[i])\n",
    "    \n",
    "def calculate_rouge(source, actual, predictions):\n",
    "    rouge = load_metric('rouge')\n",
    "    results = rouge.compute(predictions=predictions, references=actual)\n",
    "    return results['rouge1'].mid.fmeasure * 100, results['rouge2'].mid.fmeasure * 100, results['rougeL'].mid.fmeasure * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrospective Reader Performance Study on UCSF Finegrained CT Chest Dataset\n",
    "finegrained_source, finegrained_actual, finegrained_predictions, finegrained_csv = make_demo(\n",
    "    mode='finegrained', \n",
    "    reader_performance=True,\n",
    "    zsfg=False\n",
    ")\n",
    "finegrained_csv.to_csv('results/finegrained_predictions.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rouge_scores(source, actual, predictions):\n",
    "    rng = np.random.RandomState(seed=12345)\n",
    "    idx = np.arange(len(source))\n",
    "\n",
    "    rouge_1_scores = []\n",
    "    rouge_2_scores = []\n",
    "    rouge_L_scores = []\n",
    "\n",
    "    for i in tqdm.tqdm(range(200)):\n",
    "        pred_idx = rng.choice(idx, size=1000, replace=True)\n",
    "        rouge_1_score, rouge_2_score, rouge_L_score = calculate_rouge(\n",
    "            np.array(source)[pred_idx], \n",
    "            np.array(actual)[pred_idx], \n",
    "            np.array(predictions)[pred_idx]\n",
    "        )\n",
    "        rouge_1_scores.append(rouge_1_score)\n",
    "        rouge_2_scores.append(rouge_2_score)\n",
    "        rouge_L_scores.append(rouge_L_score)\n",
    "\n",
    "    def rouge_ci(rouge_scores):\n",
    "        average_score = np.mean(rouge_scores)\n",
    "        ci_lower = np.percentile(rouge_scores, 2.5)\n",
    "        ci_upper = np.percentile(rouge_scores, 97.5)\n",
    "        return average_score, (ci_lower, ci_upper)\n",
    "\n",
    "    print('ROUGE-1: ', rouge_ci(rouge_1_scores))\n",
    "    print('ROUGE-2: ', rouge_ci(rouge_2_scores))\n",
    "    print('ROUGE-L ', rouge_ci(rouge_L_scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'make_demo' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[0;32mIn [105]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ucsf_specialized_source, ucsf_specialized_actual, ucsf_specialized_predictions, _ \u001b[38;5;241m=\u001b[39m \u001b[43mmake_demo\u001b[49m(\n\u001b[1;32m      2\u001b[0m     mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mspecialized\u001b[39m\u001b[38;5;124m'\u001b[39m, \n\u001b[1;32m      3\u001b[0m     reader_performance\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, \n\u001b[1;32m      4\u001b[0m     zsfg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      7\u001b[0m rouge_scores(ucsf_specialized_source, ucsf_specialized_actual, ucsf_specialized_predictions)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'make_demo' is not defined"
     ]
    }
   ],
   "source": [
    "ucsf_specialized_source, ucsf_specialized_actual, ucsf_specialized_predictions, _ = make_demo(\n",
    "    mode='specialized', \n",
    "    reader_performance=False, \n",
    "    zsfg=False\n",
    ")\n",
    "\n",
    "rouge_scores(ucsf_specialized_source, ucsf_specialized_actual, ucsf_specialized_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/2533 [00:00<?, ?it/s]/home/bdrad/anaconda3/envs/aig/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 2533/2533 [41:07<00:00,  1.03it/s]\n",
      "100%|█████████████████████████████████████████| 200/200 [08:10<00:00,  2.45s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  (53.15717422689009, (52.175724678092216, 54.30767622103647))\n",
      "ROUGE-2:  (36.151224519851034, (34.938590412089475, 37.586254162984986))\n",
      "ROUGE-L  (45.00913662294909, (43.891346356148986, 46.319175629224326))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zsfg_specialized_source, zsfg_specialized_actual, zsfg_specialized_predictions, _ = make_demo(\n",
    "    mode='specialized', \n",
    "    reader_performance=False, \n",
    "    zsfg=True\n",
    ")\n",
    "\n",
    "rouge_scores(zsfg_specialized_source, zsfg_specialized_actual, zsfg_specialized_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/676 [00:00<?, ?it/s]/home/bdrad/anaconda3/envs/aig/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 676/676 [09:17<00:00,  1.21it/s]\n",
      "100%|█████████████████████████████████████████| 200/200 [06:44<00:00,  2.02s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  (54.66891653585523, (53.29150148565835, 55.710409186160526))\n",
      "ROUGE-2:  (38.30608612323276, (36.8242358171107, 39.73466882965169))\n",
      "ROUGE-L  (48.349445003980726, (47.08724238350272, 49.656067862239304))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ucsf_finegrained_source, ucsf_finegrained_actual, ucsf_finegrained_predictions, _ = make_demo(\n",
    "    mode='finegrained', \n",
    "    reader_performance=False, \n",
    "    zsfg=False\n",
    ")\n",
    "\n",
    "rouge_scores(ucsf_finegrained_source, ucsf_finegrained_actual, ucsf_finegrained_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/4129 [00:00<?, ?it/s]/home/bdrad/anaconda3/envs/aig/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|███████████████████████████████████████| 4129/4129 [55:52<00:00,  1.23it/s]\n",
      "100%|█████████████████████████████████████████| 200/200 [07:10<00:00,  2.15s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  (47.48753918608839, (46.325304983097865, 48.94960249538783))\n",
      "ROUGE-2:  (32.086507597541505, (30.842379664978736, 33.877684001662885))\n",
      "ROUGE-L  (40.79812033807301, (39.59751799972317, 42.29522673871863))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "zsfg_finegrained_source, zsfg_finegrained_actual, zsfg_finegrained_predictions, _ = make_demo(\n",
    "    mode='finegrained', \n",
    "    reader_performance=False, \n",
    "    zsfg=True\n",
    ")\n",
    "\n",
    "rouge_scores(zsfg_finegrained_source, zsfg_finegrained_actual, zsfg_finegrained_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/9056 [00:00<?, ?it/s]/home/bdrad/anaconda3/envs/aig/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████| 9056/9056 [2:12:05<00:00,  1.14it/s]\n",
      "100%|█████████████████████████████████████████| 200/200 [07:32<00:00,  2.26s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  (53.68045537449058, (52.456244872353686, 54.78902986414531))\n",
      "ROUGE-2:  (36.5657392180976, (35.15058140640832, 37.85500384762911))\n",
      "ROUGE-L  (46.25484175355792, (44.89300526951312, 47.4177312008427))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ucsf_general_ct_source, ucsf_general_ct_actual, ucsf_general_ct_predictions, _ = make_demo(\n",
    "    mode='general', \n",
    "    department='CT',\n",
    "    reader_performance=False, \n",
    "    zsfg=False\n",
    ")\n",
    "\n",
    "rouge_scores(ucsf_general_ct_source, ucsf_general_ct_actual, ucsf_general_ct_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/6541 [00:00<?, ?it/s]/home/bdrad/anaconda3/envs/aig/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████| 6541/6541 [1:40:29<00:00,  1.08it/s]\n",
      "100%|█████████████████████████████████████████| 200/200 [07:53<00:00,  2.37s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  (52.883550319109666, (51.65695545613904, 54.02496770060186))\n",
      "ROUGE-2:  (35.579871891910514, (34.22917920283988, 36.895173674077775))\n",
      "ROUGE-L  (45.32954247702376, (44.025573517428846, 46.51066072290612))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ucsf_general_mri_source, ucsf_general_mri_actual, ucsf_general_mri_predictions, _ = make_demo(\n",
    "    mode='general', \n",
    "    department='MR',\n",
    "    reader_performance=False, \n",
    "    zsfg=False\n",
    ")\n",
    "\n",
    "rouge_scores(ucsf_general_mri_source, ucsf_general_mri_actual, ucsf_general_mri_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                   | 0/806 [00:00<?, ?it/s]/home/bdrad/anaconda3/envs/aig/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████████| 806/806 [13:10<00:00,  1.02it/s]\n",
      "100%|█████████████████████████████████████████| 200/200 [10:24<00:00,  3.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  (54.3614604157555, (53.37464253634127, 55.316897011276765))\n",
      "ROUGE-2:  (37.57389995552365, (36.41930423297943, 38.62273765647115))\n",
      "ROUGE-L  (47.36978345747333, (46.316367269375604, 48.38511395898861))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ucsf_general_pet_source, ucsf_general_pet_actual, ucsf_general_pet_predictions, _ = make_demo(\n",
    "    mode='general', \n",
    "    department='PETCT',\n",
    "    reader_performance=False, \n",
    "    zsfg=False\n",
    ")\n",
    "\n",
    "rouge_scores(ucsf_general_pet_source, ucsf_general_pet_actual, ucsf_general_pet_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                  | 0/4812 [00:00<?, ?it/s]/home/bdrad/anaconda3/envs/aig/lib/python3.9/site-packages/transformers/tokenization_utils_base.py:2263: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "100%|█████████████████████████████████████| 4812/4812 [1:02:58<00:00,  1.27it/s]\n",
      "100%|█████████████████████████████████████████| 200/200 [07:18<00:00,  2.19s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ROUGE-1:  (53.0849076225571, (51.88031136862934, 54.391613379452295))\n",
      "ROUGE-2:  (35.51989169615126, (34.12807167840848, 37.01093802609764))\n",
      "ROUGE-L  (46.58201805394969, (45.33002060827956, 48.04583507342392))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "ucsf_general_us_source, ucsf_general_us_actual, ucsf_general_us_predictions, _ = make_demo(\n",
    "    mode='general', \n",
    "    department='US',\n",
    "    reader_performance=False, \n",
    "    zsfg=False\n",
    ")\n",
    "\n",
    "rouge_scores(ucsf_general_us_source, ucsf_general_us_actual, ucsf_general_us_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exam\n",
      "CT ABDOMEN/PELVIS WITH CONTRAST\n",
      "Clinical History\n",
      "50 y/o patient with metastatic melanoma, required to begin cancer treatment\n",
      "Comparison\n",
      "2/10/2021\n",
      "Findings\n",
      "Visualized lung bases:  For chest findings, please see the separately dictated report from the CT of the chest of the same date.\n",
      "Liver:  No focal suspicious liver lesions. .\n",
      "Gallbladder: Unremarkable\n",
      "Spleen:  Unremarkable\n",
      "Pancreas:  Unremarkable \n",
      "Adrenal Glands:  Small bilateral adrenal nodules measuring up to 13 mm on the right and a 10 mm on the left. As prior CT portion of the PET/CT was noncontrast and was for attenuation correction only, it is unclear whether these nodules were present on the prior study however the current imaging appearance is suspicious for metastatic disease.\n",
      "Kidneys:  Unremarkable\n",
      "GI Tract:  Scattered colonic diverticula without evidence of diverticulitis.\n",
      "Vasculature:  Unremarkable\n",
      "Lymphadenopathy: Absent\n",
      "Peritoneum: No ascites\n",
      "Bladder: Unremarkable\n",
      "Reproductive organs: Unremarkable\n",
      "Bones:  No suspicious lesions\n",
      "Extraperitoneal soft tissues: Unremarkable\n",
      "Lines/drains/medical devices: None\n",
      "\n",
      "Impression\n",
      "1. Small bilateral adrenal nodules measuring up to 13 mm on the right and 10 mm on the left. As prior CT portion of the PET/CT was noncontrast and was for attenuation correction only, it is unclear whether these nodules were present on the prior study. However these may be new and suspicious for metastatic disease.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_114906/2868388783.py:3: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n",
      "  row = train[train['Impression'].str.contains('metastatic')][train['Exam'].str.startswith('CT')].iloc[85]\n"
     ]
    }
   ],
   "source": [
    "# train = pd.read_csv('data/processed/general_train_dataset.csv')\n",
    "train = train.dropna()\n",
    "row = train[train['Impression'].str.contains('metastatic')][train['Exam'].str.startswith('CT')].iloc[85]\n",
    "print('Exam')\n",
    "print(row['Exam'])\n",
    "print('Clinical History')\n",
    "print(row['Clinical History'])\n",
    "print('Comparison')\n",
    "print(row['Comparison'])\n",
    "print('Findings')\n",
    "print(row['Findings'])\n",
    "print('Impression')\n",
    "print(row['Impression'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "secure_UCSF_CT_2022-05-01_2022-07-31.xlsx\r\n",
      "secure_UCSF_CT_CHEST_2021-09-01-to-2022-09-02.xlsx\r\n",
      "secure_UCSF_MR16k_2022-05-01_2022-07-31.xlsx\r\n",
      "secure_UCSF_PET_2029-05-01-2022-07-31.xlsx\r\n",
      "secure_UCSF_radreports__CT_MR_MRI_US_PET__01-01-2021__10-22-2022.csv\r\n",
      "secure_UCSF_US15k_2022-05-01-2022-07-31.xlsx\r\n"
     ]
    }
   ],
   "source": [
    "!ls data/raw\n",
    "os.listdir('data/raw')\n",
    "raw = pd.read_excel('data/raw/secure_UCSF_CT_2022-05-01_2022-07-31.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw = pd.read_csv('data/raw/secure_UCSF_radreports__CT_MR_MRI_US_PET__01-01-2021__10-22-2022.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EXAMINATION DESCRIPTION:\n",
      "CT LUMBAR SPINE WITHOUT CONTRAST\n",
      "INDICATION(S): \n",
      "please scan pedicles of L4-S2 assess fusion L5-s1  status post anterior lumbar interbody fusion and posterior fusion at L5-S1\n",
      "SEDATION:\n",
      "None.\n",
      "TECHNIQUE:\n",
      "Helical CT scan of the lower lumbar spine was performed without intravenous contrast administration, with metallic artifact reduction technique.\n",
      "CTDI/DLP:\n",
      "CTDI: Exposure Events: 2 , CTDIvol Min: 0 mGy, CTDIvol Max: 5.5 mGy, DLP: 65 mGy.cm \n",
      "COMPARISON:\n",
      "No previous studies are available for comparison.\n",
      "FINDINGS:\n",
      "As requested, lumbar spine CT scan was performed from L4 to the level S2. There is anterior lumbar interbody fusion and posterior fusion noted at the level L5-S1 with a pair of pedicle screw from a posterior fusion and then a anterior single anchoring screw of the interbody to the L5 vertebral body and a pair of anterior screw to the S1 vertebral body. Alignment of L5-S1 is intact. There are no lucencies noted around the interbody fusion. Along the posterior element on the right side L5, there is bony fusion mass noted predominantly along the right side lamina, not fully incorporated at this time. Please correlate clinically.\n",
      "\n",
      "Status post anterior lumbar interbody fusion and posterior bony fusion at level L5/S1 as described.\n",
      "Report dictated by: Taylor Chung, MD, signed by: Taylor Chung, MD\n",
      "Department of Radiology and Biomedical Imaging\n"
     ]
    }
   ],
   "source": [
    "print(raw.iloc[164]['Report Text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
